# Projeto Final - Engenharia de Dados | Data Girls ğŸ‘©ğŸ¾â€ğŸ’»

Este projeto tem como objetivo aplicar os conhecimentos adquiridos no bootcamp de Engenharia de Dados da comunidade [Data Girls](https://www.datagirls.com.br/), utilizando ferramentas modernas para desenvolver um pipeline ETL completo, com orquestraÃ§Ã£o via Apache Airflow, armazenamento no Google Cloud Storage, anÃ¡lise no BigQuery e visualizaÃ§Ãµes com Looker Studio.

## ğŸ” DescriÃ§Ã£o

O projeto utiliza o dataset [IBM HR Analytics Employee Attrition & Performance](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-performance), que trata da rotatividade de funcionÃ¡rios com base em dados de RH. A proposta Ã© extrair, transformar e carregar esses dados em um bucket do GCP e posteriormente no BigQuery, a fim de gerar anÃ¡lises e insights por meio de visualizaÃ§Ãµes.

---

## ğŸ§± Tecnologias utilizadas

- **Python 3.10** â€“ manipulaÃ§Ã£o e transformaÃ§Ã£o de dados
- **Pandas** â€“ tratamento e traduÃ§Ã£o dos dados
- **Apache Airflow 2.7.2** â€“ orquestraÃ§Ã£o de tarefas com `docker-compose`
- **Docker Desktop 4.42.1** â€“ containerizaÃ§Ã£o e execuÃ§Ã£o local
- **Google Cloud Platform (GCP)** â€“ armazenamento e anÃ¡lise dos dados
  - Google Cloud Storage
  - BigQuery
- **Looker Studio** â€“ visualizaÃ§Ãµes e dashboard interativo

---

## ğŸ—‚ï¸ OrganizaÃ§Ã£o do Projeto

```
datagirls_projetofinal/
â”œâ”€â”€ airflowdocker/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ etl_datagirlspfinal.py
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â””â”€â”€ load_to_bigquery.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ docker-compose.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ dados_transformados.parquet
â”œâ”€â”€ chaves/
â”‚   â”œâ”€â”€ kaggle.json
â”‚   â””â”€â”€ gcp_service_account.json
â””â”€â”€ README.md
```

---

## âš™ï¸ Pipeline ETL

O pipeline Ã© composto por quatro etapas, agendadas para rodar diariamente (`@daily`):

1. **`extract.py`**: Faz o download automÃ¡tico do dataset no Kaggle.
2. **`transform.py`**: Remove colunas irrelevantes, trata nulos e traduz todas as colunas e valores para portuguÃªs.
3. **`load.py`**: Envia os dados transformados (`.parquet`) para o bucket no GCP.
4. **`load_to_bigquery.py`**: Carrega o arquivo `.parquet` do bucket para o BigQuery, dentro do conjunto `carbide-eye-466719-s1.datagirls_projetofinal`.

---

## ğŸ”¤ TraduÃ§Ã£o dos dados

A etapa de transformaÃ§Ã£o inclui:

- TraduÃ§Ã£o de colunas como `Age` â†’ `idade`, `JobRole` â†’ `cargo`, etc.
- TraduÃ§Ã£o de valores categÃ³ricos como:
  - `BusinessTravel`: `"travel_rarely"` â†’ `"raramente"`, `"travel_frequently"` â†’ `"frequentemente"`, `"non_travel"` â†’ `"nÃ£o viaja"`
  - `Department`: `"sales"` â†’ `"vendas"`, `"research & development"` â†’ `"pesquisa e desenvolvimento"`
  - `MaritalStatus`: `"single"` â†’ `"solteiro(a)"`, `"married"` â†’ `"casado(a)"`, `"divorced"` â†’ `"divorciado(a)"`
  - `JobRole`, `EducationField`, entre outras, foram todas traduzidas

---

## â˜ï¸ GCP

- **Bucket criado:** `etl_datagirlspfinal`
- **Projeto:** `carbide-eye-466719-s1`
- **Conjunto de dados no BigQuery:** `datagirls_projetofinal`
- **Tabela:** `dados_transformados`

---

## ğŸ“Š Dashboard

A etapa de BI estÃ¡ sendo desenvolvida com **Looker Studio**, utilizando a tabela carregada no BigQuery. O dashboard exibirÃ¡ mÃ©tricas sobre:

- Rotatividade de funcionÃ¡rios (Attrition)
- Perfil dos colaboradores por Ã¡rea, cargo, tempo de empresa, entre outros

*Link em breve...*

---

## âœ… Status do Projeto

- [x] ConfiguraÃ§Ã£o do ambiente Docker + Airflow
- [x] Script de extraÃ§Ã£o via API do Kaggle
- [x] TransformaÃ§Ã£o e traduÃ§Ã£o completa dos dados
- [x] Upload para o bucket no GCP
- [x] Carga no BigQuery
- [ ] CriaÃ§Ã£o do dashboard Looker Studio

---

## ğŸ’¡ Aprendizados

Durante o desenvolvimento deste projeto, foram consolidados conhecimentos em:

- OrquestraÃ§Ã£o de pipelines com Airflow
- UtilizaÃ§Ã£o da Kaggle API
- ManipulaÃ§Ã£o de dados com Pandas
- IntegraÃ§Ã£o com GCP (Storage + BigQuery)
- OrganizaÃ§Ã£o de projeto de dados com boas prÃ¡ticas

---

## ğŸ¤ Agradecimentos

Este projeto foi desenvolvido por **Marcela** durante o bootcamp da [Data Girls](https://www.datagirls.com.br/). Agradecimentos especiais Ã s instrutoras e Ã  comunidade pela partilha de conhecimento e apoio contÃ­nuo. ğŸ’œ

---

## ğŸ“ LicenÃ§a

Este projeto Ã© apenas para fins educacionais e nÃ£o possui fins comerciais.