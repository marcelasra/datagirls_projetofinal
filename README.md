# ğŸ“Š Projeto ETL â€“ DataGirls Projeto Final

Pipeline completo de engenharia de dados desenvolvido com foco em automaÃ§Ã£o, transformaÃ§Ã£o e visualizaÃ§Ã£o dos dados de RH da IBM. O objetivo Ã© monitorar mÃ©tricas de rotatividade e fornecer insights valiosos para a Ã¡rea de Recursos Humanos.

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ airflowdocker/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ etl_datagirlspfinal.py
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â””â”€â”€ load.py
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ chaves/
â”‚   â”œâ”€â”€ kaggle.json
â”‚   â””â”€â”€ gcp_service_account.json
```

## ğŸš€ Tecnologias Utilizadas

- **Apache Airflow** v2.7.2
- **Docker** + Docker Compose
- **Python** v3.10
- **Pandas** e **PyArrow**
- **Kaggle API**
- **Google Cloud Storage (GCS)**
- **BigQuery (planejado)**
- **Power BI ou Looker Studio (para visualizaÃ§Ã£o)**

## âš™ï¸ ExecuÃ§Ã£o do Pipeline

### PrÃ©-requisitos

- Docker + Docker Compose
- Conta no [Kaggle](https://www.kaggle.com/) e chave `kaggle.json`
- Conta e bucket criado no GCP

### Passo a passo

1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/seuusuario/datagirls_projetofinal.git
   cd datagirls_projetofinal/airflowdocker
   ```

2. Execute:
   ```bash
   docker compose up --build
   ```

3. Acesse o Airflow via [http://localhost:8080](http://localhost:8080)  
   UsuÃ¡rio padrÃ£o: `airflow`  
   Senha: `airflow`

4. Inicie manualmente a DAG `etl_datagirlspfinal`.

## ğŸ§  LÃ³gica do ETL

### ExtraÃ§Ã£o (`extract.py`)
- A API do Kaggle baixa o dataset de RH da IBM.
- Os dados sÃ£o descompactados e salvos em `data/raw`.

### TransformaÃ§Ã£o (`transform.py`)
- RemoÃ§Ã£o de colunas irrelevantes como `EmployeeCount`, `Over18`, `StandardHours`.
- PadronizaÃ§Ã£o dos nomes das colunas (`snake_case`).
- ConversÃ£o para `lowercase`.
- RemoÃ§Ã£o de nulos e duplicados.
- Dados transformados sÃ£o salvos em `.parquet` em `data/processed`.

### Carga (`load.py`)
- O arquivo `dados_transformados.parquet` Ã© enviado para o bucket `etl_datagirlspfinal` no GCS.
- O script utiliza a conta de serviÃ§o do GCP via `google-cloud-storage`.

## â˜ï¸ IntegraÃ§Ãµes Finais (em desenvolvimento)

- Os dados carregados no GCS serÃ£o ingeridos no **BigQuery**.
- As visualizaÃ§Ãµes serÃ£o criadas via **Looker Studio** (gratuito) ou **Power BI**.

## â“Perguntas Norteadoras de NegÃ³cio

1. **Como a empresa pode monitorar a rotatividade de funcionÃ¡rios semanalmente?**  
   â†’ Com execuÃ§Ã£o diÃ¡ria da DAG e integraÃ§Ã£o com o BI, Ã© possÃ­vel gerar dashboards semanais com base nas saÃ­das registradas.

2. **Quais informaÃ§Ãµes devem ser atualizadas em tempo real ou periodicamente?**  
   â†’ Neste projeto, a periodicidade ideal Ã© diÃ¡ria, suficiente para detectar tendÃªncias de rotatividade com agilidade.

3. **Como garantir que os dados estejam prontos para anÃ¡lises de forma confiÃ¡vel?**  
   â†’ A transformaÃ§Ã£o remove ruÃ­dos, padroniza nomes e garante consistÃªncia estrutural, alÃ©m de exportar para um formato leve (.parquet).

4. **Ã‰ possÃ­vel criar um modelo incremental com essa base?**  
   â†’ Sim, adaptando a lÃ³gica de extraÃ§Ã£o para controlar modificaÃ§Ãµes e utilizando marcas temporais (timestamps).

## ğŸ“Œ ObservaÃ§Ãµes

- O pipeline Ã© modular e preparado para escala.
- A DAG `etl_datagirlspfinal` pode ser ajustada para ingestÃµes mais frequentes ou acionadas por eventos.

## ğŸ™ Agradecimentos

Este projeto foi desenvolvido por Marcela como parte do bootcamp de Engenharia de Dados promovido pela **comunidade Data Girls**.  
Agradecimentos especiais Ã s instrutoras e colegas que contribuÃ­ram ao longo do processo.